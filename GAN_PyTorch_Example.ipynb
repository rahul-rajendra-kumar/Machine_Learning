{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_PyTorch_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCov0a1cbDHXHkdGqeHJZk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-rajendra-kumar/Machine_Learning/blob/main/GAN_PyTorch_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtwSF59vXhlm"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYiBGctIdP_d",
        "outputId": "2c110e4b-d22a-43f2-83bb-05dd3e26e19f"
      },
      "source": [
        "# Data params\n",
        "data_mean = 4\n",
        "data_stddev = 1.25\n",
        "\n",
        "# Uncomment only one of these to define what data is actually sent to the Discriminator\n",
        "\n",
        "(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
        "#(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
        "#(name, preprocess, d_input_func) = (\"Data and diffs\", lambda data: decorate_with_diffs(data, 1.0), lambda x: x * 2)\n",
        "# (name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n",
        "\n",
        "print(\"Using data [%s]\" % (name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using data [Raw data]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKMTebyrdaYI"
      },
      "source": [
        "# ##### DATA: Target data and generator input data\n",
        "\n",
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian\n",
        "\n",
        "def get_generator_input_sampler():\n",
        "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0PldH-tbkoR"
      },
      "source": [
        "# ##### MODELS: Generator model and discriminator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.map1(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map2(x)\n",
        "        x = self.f(x)\n",
        "        x = self.map3(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, f):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(self.map1(x))\n",
        "        x = self.f(self.map2(x))\n",
        "        return self.f(self.map3(x))\n",
        "        \n",
        "def extract(v):\n",
        "    return v.data.storage().tolist()\n",
        "\n",
        "def stats(d):\n",
        "    return [np.mean(d), np.std(d)]\n",
        "\n",
        "def get_moments(d):\n",
        "    # Return the first 4 moments of the data provided\n",
        "    mean = torch.mean(d)\n",
        "    diffs = d - mean\n",
        "    var = torch.mean(torch.pow(diffs, 2.0))\n",
        "    std = torch.pow(var, 0.5)\n",
        "    zscores = diffs / std\n",
        "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
        "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
        "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
        "    return final\n",
        "\n",
        "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
        "    mean = torch.mean(data.data, 1, keepdim=True)\n",
        "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
        "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
        "    if remove_raw_data:\n",
        "        return torch.cat([diffs], 1)\n",
        "    else:\n",
        "        return torch.cat([data, diffs], 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QLMlC6TSdtKm",
        "outputId": "78889e30-f4cb-40cf-c375-d8ab6a509d90"
      },
      "source": [
        "def train():\n",
        "    # Model parameters\n",
        "    g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
        "    g_hidden_size = 5     # Generator complexity\n",
        "    g_output_size = 1     # Size of generated output vector\n",
        "    d_input_size = 500    # Minibatch size - cardinality of distributions\n",
        "    d_hidden_size = 10    # Discriminator complexity\n",
        "    d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
        "    minibatch_size = d_input_size\n",
        "\n",
        "    d_learning_rate = 1e-3\n",
        "    g_learning_rate = 1e-3\n",
        "    sgd_momentum = 0.9\n",
        "\n",
        "    num_epochs = 5000\n",
        "    print_interval = 100\n",
        "    d_steps = 20\n",
        "    g_steps = 20\n",
        "\n",
        "    dfe, dre, ge = 0, 0, 0\n",
        "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
        "\n",
        "    discriminator_activation_function = torch.sigmoid\n",
        "    generator_activation_function = torch.tanh\n",
        "\n",
        "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "    gi_sampler = get_generator_input_sampler()\n",
        "    G = Generator(input_size=g_input_size,\n",
        "                  hidden_size=g_hidden_size,\n",
        "                  output_size=g_output_size,\n",
        "                  f=generator_activation_function)\n",
        "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
        "                      hidden_size=d_hidden_size,\n",
        "                      output_size=d_output_size,\n",
        "                      f=discriminator_activation_function)\n",
        "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
        "    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for d_index in range(d_steps):\n",
        "            # 1. Train D on real+fake\n",
        "            D.zero_grad()\n",
        "\n",
        "            #  1A: Train D on real\n",
        "            d_real_data = Variable(d_sampler(d_input_size))\n",
        "            d_real_decision = D(preprocess(d_real_data))\n",
        "            d_real_error = criterion(d_real_decision, Variable(torch.ones([1,1])))  # ones = true\n",
        "            d_real_error.backward() # compute/store gradients, but don't change params\n",
        "\n",
        "            #  1B: Train D on fake\n",
        "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
        "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros([1,1])))  # zeros = fake\n",
        "            d_fake_error.backward()\n",
        "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "\n",
        "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
        "\n",
        "        for g_index in range(g_steps):\n",
        "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
        "            G.zero_grad()\n",
        "\n",
        "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "            g_fake_data = G(gen_input)\n",
        "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
        "            g_error = criterion(dg_fake_decision, Variable(torch.ones([1,1])))  # Train G to pretend it's genuine\n",
        "\n",
        "            g_error.backward()\n",
        "            g_optimizer.step()  # Only optimizes G's parameters\n",
        "            ge = extract(g_error)[0]\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(\"Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) \" %\n",
        "                  (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
        "\n",
        "    print(\"Plotting the generated distribution...\")\n",
        "    values = extract(g_fake_data)\n",
        "    print(\" Values: %s\" % (str(values)))\n",
        "    plt.hist(values, bins=50)\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Histogram of Generated Distribution')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: D (0.7731191515922546 real_err, 0.6128182411193848 fake_err) G (0.778140127658844 err); Real Dist ([3.977260496631265, 1.2569313372794684]),  Fake Dist ([0.2084446423947811, 0.020825063241150268]) \n",
            "Epoch 100: D (0.7410588264465332 real_err, 0.6587995886802673 fake_err) G (0.7291269898414612 err); Real Dist ([3.926752114377916, 1.2671652344843145]),  Fake Dist ([3.6533077511787413, 0.008053091232657022]) \n",
            "Epoch 200: D (0.6959811449050903 real_err, 0.6885963678359985 fake_err) G (0.6975634098052979 err); Real Dist ([3.9857086038589475, 1.2234162252713345]),  Fake Dist ([3.8315568690299986, 0.006565159639820851]) \n",
            "Epoch 300: D (0.6922755241394043 real_err, 0.6931673288345337 fake_err) G (0.6931222677230835 err); Real Dist ([4.02492946061492, 1.253280957208535]),  Fake Dist ([3.885946764945984, 0.0059256254068459045]) \n",
            "Epoch 400: D (0.7020921111106873 real_err, 0.7327190637588501 fake_err) G (0.6701878309249878 err); Real Dist ([4.0974140733480455, 1.1690056754990255]),  Fake Dist ([3.804736319065094, 0.007026851361634415]) \n",
            "Epoch 500: D (0.7092241644859314 real_err, 0.7063143253326416 fake_err) G (0.6722318530082703 err); Real Dist ([3.9731402436494827, 1.2675622863121747]),  Fake Dist ([3.839331654548645, 0.006886045692130565]) \n",
            "Epoch 600: D (0.7169947028160095 real_err, 0.6578837037086487 fake_err) G (0.7314924001693726 err); Real Dist ([4.008122508019209, 1.3002847938549598]),  Fake Dist ([3.848147624492645, 0.006554316042045022]) \n",
            "Epoch 700: D (0.6911928653717041 real_err, 0.6919595003128052 fake_err) G (0.694301426410675 err); Real Dist ([3.944884521484375, 1.2639512171586371]),  Fake Dist ([3.8367827715873717, 0.006851321399196789]) \n",
            "Epoch 800: D (0.7016869783401489 real_err, 0.6842033267021179 fake_err) G (0.701793372631073 err); Real Dist ([3.9684515993520617, 1.2576093059541553]),  Fake Dist ([3.845269862174988, 0.0075030413999721424]) \n",
            "Epoch 900: D (0.6932549476623535 real_err, 0.6931655406951904 fake_err) G (0.6931279897689819 err); Real Dist ([4.04714454138279, 1.2429775031847796]),  Fake Dist ([3.9810927309989927, 0.0063696815892476475]) \n",
            "Epoch 1000: D (0.6931337118148804 real_err, 0.6931601762771606 fake_err) G (0.6931334733963013 err); Real Dist ([4.0976914591491225, 1.2280646528021226]),  Fake Dist ([4.014849013328552, 0.006619960565606149]) \n",
            "Epoch 1100: D (0.6931545734405518 real_err, 0.6931527853012085 fake_err) G (0.6931419372558594 err); Real Dist ([4.012234494805336, 1.234683118873257]),  Fake Dist ([4.031900832176208, 0.005998078045356608]) \n",
            "Epoch 1200: D (0.694844126701355 real_err, 0.6914496421813965 fake_err) G (0.6947468519210815 err); Real Dist ([4.083136399976909, 1.2361680139789057]),  Fake Dist ([3.977663166046143, 0.006518576578554471]) \n",
            "Epoch 1300: D (0.6930694580078125 real_err, 0.6931257247924805 fake_err) G (0.6931681632995605 err); Real Dist ([4.028089753627777, 1.309601759571671]),  Fake Dist ([3.762938343048096, 0.009881654468144025]) \n",
            "Epoch 1400: D (0.6931238174438477 real_err, 0.6930735111236572 fake_err) G (0.6932163238525391 err); Real Dist ([3.9704857442975046, 1.3290672559312529]),  Fake Dist ([3.8041227169036866, 0.008783843527604307]) \n",
            "Epoch 1500: D (0.6933459639549255 real_err, 0.6931853294372559 fake_err) G (0.6931089162826538 err); Real Dist ([3.992194066762924, 1.1953671737984637]),  Fake Dist ([3.930118637084961, 0.007655915589462109]) \n",
            "Epoch 1600: D (0.6930750608444214 real_err, 0.6931594610214233 fake_err) G (0.6931347846984863 err); Real Dist ([4.0709128103256225, 1.2514801396863522]),  Fake Dist ([3.978327231884003, 0.00725467159088277]) \n",
            "Epoch 1700: D (0.6931719779968262 real_err, 0.6931544542312622 fake_err) G (0.6931396722793579 err); Real Dist ([3.989377666153014, 1.2885083382060398]),  Fake Dist ([4.003229889392853, 0.006900027908465291]) \n",
            "Epoch 1800: D (0.6931989192962646 real_err, 0.6931567192077637 fake_err) G (0.693137526512146 err); Real Dist ([3.8984489989876745, 1.2060915022695717]),  Fake Dist ([4.019214818954468, 0.0065541720626249205]) \n",
            "Epoch 1900: D (0.6931536197662354 real_err, 0.6931548118591309 fake_err) G (0.6931396722793579 err); Real Dist ([3.90244200360775, 1.1813107233367346]),  Fake Dist ([4.029723466873169, 0.006714141359502172]) \n",
            "Epoch 2000: D (0.6931555271148682 real_err, 0.6931524276733398 fake_err) G (0.6931419372558594 err); Real Dist ([4.001549620449543, 1.246286870922583]),  Fake Dist ([4.037986354827881, 0.0068357583122259135]) \n",
            "Epoch 2100: D (0.693156361579895 real_err, 0.6931507587432861 fake_err) G (0.6931436061859131 err); Real Dist ([4.009017731904984, 1.1994787838791507]),  Fake Dist ([4.044552849769592, 0.0067312368218753]) \n",
            "Epoch 2200: D (0.6931442022323608 real_err, 0.6931507587432861 fake_err) G (0.6931436061859131 err); Real Dist ([4.0304727278053765, 1.3225359825126715]),  Fake Dist ([4.050175246238709, 0.006525279149432314]) \n",
            "Epoch 2300: D (0.693142294883728 real_err, 0.6931502819061279 fake_err) G (0.6931442022323608 err); Real Dist ([4.054957427173853, 1.2280051680882431]),  Fake Dist ([4.05447987651825, 0.006556248802828255]) \n",
            "Epoch 2400: D (0.693152666091919 real_err, 0.6931499242782593 fake_err) G (0.6931445598602295 err); Real Dist ([4.053095610976219, 1.2911928918000446]),  Fake Dist ([4.058226865768432, 0.006665579353674137]) \n",
            "Epoch 2500: D (0.6931575536727905 real_err, 0.6931494474411011 fake_err) G (0.6931450366973877 err); Real Dist ([3.9033010335862635, 1.2329652777163493]),  Fake Dist ([4.061749488830566, 0.006553914754635577]) \n",
            "Epoch 2600: D (0.6931507587432861 real_err, 0.6931489706039429 fake_err) G (0.6931453943252563 err); Real Dist ([3.9253597413003445, 1.334306992847548]),  Fake Dist ([4.064397565841674, 0.006576538948298827]) \n",
            "Epoch 2700: D (0.6931512355804443 real_err, 0.6931489706039429 fake_err) G (0.6931453943252563 err); Real Dist ([3.9959000811576844, 1.239169590407367]),  Fake Dist ([4.0673886547088625, 0.006479690128529211]) \n",
            "Epoch 2800: D (0.693148136138916 real_err, 0.693149209022522 fake_err) G (0.6931451559066772 err); Real Dist ([4.078477827072144, 1.2023233850614545]),  Fake Dist ([4.069616500854492, 0.00675755347038848]) \n",
            "Epoch 2900: D (0.6931414604187012 real_err, 0.6931486129760742 fake_err) G (0.6931456327438354 err); Real Dist ([3.9734070519804954, 1.2190917700504793]),  Fake Dist ([4.071526378631591, 0.00662021304263923]) \n",
            "Epoch 3000: D (0.6931452751159668 real_err, 0.6931488513946533 fake_err) G (0.6931455135345459 err); Real Dist ([4.093499543786049, 1.2335177859144821]),  Fake Dist ([4.073523417472839, 0.0063520778375095455]) \n",
            "Epoch 3100: D (0.6931474208831787 real_err, 0.6931482553482056 fake_err) G (0.6931461095809937 err); Real Dist ([4.015639332652092, 1.254901837016043]),  Fake Dist ([4.075452107429505, 0.006494239125209702]) \n",
            "Epoch 3200: D (0.6931475400924683 real_err, 0.6931484937667847 fake_err) G (0.6931458711624146 err); Real Dist ([4.071925293326378, 1.2090529716870173]),  Fake Dist ([4.0771819000244145, 0.006510451539095725]) \n",
            "Epoch 3300: D (0.6931567192077637 real_err, 0.6931482553482056 fake_err) G (0.6931461095809937 err); Real Dist ([3.9394956442117692, 1.2884632658083488]),  Fake Dist ([4.0793830022811886, 0.0062359513490456]) \n",
            "Epoch 3400: D (0.6931486129760742 real_err, 0.693148136138916 fake_err) G (0.6931461095809937 err); Real Dist ([3.998501839391887, 1.3105435054940529]),  Fake Dist ([4.080447101593018, 0.00644387703857426]) \n",
            "Epoch 3500: D (0.6931540966033936 real_err, 0.693148136138916 fake_err) G (0.6931462287902832 err); Real Dist ([3.9402113850712777, 1.2617530520059452]),  Fake Dist ([4.081787087440491, 0.0065129088142353]) \n",
            "Epoch 3600: D (0.6931428909301758 real_err, 0.6931480169296265 fake_err) G (0.6931463479995728 err); Real Dist ([3.9571535813808443, 1.2417615735526102]),  Fake Dist ([4.083064381599426, 0.006331485291644697]) \n",
            "Epoch 3700: D (0.6931536197662354 real_err, 0.693148136138916 fake_err) G (0.6931462287902832 err); Real Dist ([4.0054324884712695, 1.2078991946334183]),  Fake Dist ([4.084652333259583, 0.006338853510976579]) \n",
            "Epoch 3800: D (0.6931476593017578 real_err, 0.6931480169296265 fake_err) G (0.6931463479995728 err); Real Dist ([4.022305924594402, 1.189012816664547]),  Fake Dist ([4.085649966239929, 0.006351433861863277]) \n",
            "Epoch 3900: D (0.6931531429290771 real_err, 0.6931480169296265 fake_err) G (0.6931463479995728 err); Real Dist ([3.9586391715705394, 1.287068913684368]),  Fake Dist ([4.08697099685669, 0.0063101662983605]) \n",
            "Epoch 4000: D (0.693146824836731 real_err, 0.6931478977203369 fake_err) G (0.6931464672088623 err); Real Dist ([4.024786447763443, 1.2596947302494015]),  Fake Dist ([4.087744268417358, 0.006348111671094431]) \n",
            "Epoch 4100: D (0.6931502819061279 real_err, 0.6931477785110474 fake_err) G (0.6931464672088623 err); Real Dist ([3.9167229533195496, 1.2527231144623516]),  Fake Dist ([4.089493941307068, 0.006322976188475455]) \n",
            "Epoch 4200: D (0.693150520324707 real_err, 0.6931478977203369 fake_err) G (0.6931464672088623 err); Real Dist ([3.9669441939294336, 1.2381442871294694]),  Fake Dist ([4.090656759262085, 0.00604785212843461]) \n",
            "Epoch 4300: D (0.6931488513946533 real_err, 0.6931480169296265 fake_err) G (0.6931463479995728 err); Real Dist ([4.0826550805568695, 1.2139910887700969]),  Fake Dist ([4.091755699157715, 0.006057853404738282]) \n",
            "Epoch 4400: D (0.6931495666503906 real_err, 0.6931477785110474 fake_err) G (0.6931464672088623 err); Real Dist ([3.977023852407932, 1.2082494838170055]),  Fake Dist ([4.093079940795898, 0.0062974446957805765]) \n",
            "Epoch 4500: D (0.6931471824645996 real_err, 0.6931478977203369 fake_err) G (0.6931464672088623 err); Real Dist ([4.073086595542729, 1.1889806671995822]),  Fake Dist ([4.0931721715927125, 0.006563867462908019]) \n",
            "Epoch 4600: D (0.693150520324707 real_err, 0.6931478977203369 fake_err) G (0.6931464672088623 err); Real Dist ([3.9957023932933806, 1.2622177714434601]),  Fake Dist ([4.094430575370788, 0.00608855196665591]) \n",
            "Epoch 4700: D (0.6931496858596802 real_err, 0.6931478977203369 fake_err) G (0.6931464672088623 err); Real Dist ([3.9850779960155487, 1.2160611840931015]),  Fake Dist ([4.09552338886261, 0.005986902858124734]) \n",
            "Epoch 4800: D (0.6931474208831787 real_err, 0.6931477785110474 fake_err) G (0.6931464672088623 err); Real Dist ([3.9897185839414595, 1.27723874029044]),  Fake Dist ([4.095333636283875, 0.006391910763658893]) \n",
            "Epoch 4900: D (0.6931498050689697 real_err, 0.6931478977203369 fake_err) G (0.6931464672088623 err); Real Dist ([3.8998139443695545, 1.2496880678671762]),  Fake Dist ([4.095618873596192, 0.006614019842235552]) \n",
            "Plotting the generated distribution...\n",
            " Values: [4.097355365753174, 4.101746082305908, 4.095602512359619, 4.09372615814209, 4.102259635925293, 4.09821891784668, 4.097768306732178, 4.101217269897461, 4.102123737335205, 4.083981990814209, 4.092016696929932, 4.09993314743042, 4.094594955444336, 4.080191612243652, 4.101919174194336, 4.102260112762451, 4.102068901062012, 4.101584434509277, 4.100504398345947, 4.095848083496094, 4.088628768920898, 4.102294445037842, 4.102259635925293, 4.100661754608154, 4.101635932922363, 4.09840202331543, 4.090127944946289, 4.099148750305176, 4.100504398345947, 4.102222442626953, 4.1023054122924805, 4.102059364318848, 4.101105213165283, 4.101590156555176, 4.088372707366943, 4.0988311767578125, 4.086187362670898, 4.093557834625244, 4.0932841300964355, 4.0966033935546875, 4.08417272567749, 4.090120315551758, 4.101937294006348, 4.102002143859863, 4.099253177642822, 4.098112106323242, 4.1003546714782715, 4.1007490158081055, 4.097720146179199, 4.1022186279296875, 4.102299690246582, 4.100818157196045, 4.089789390563965, 4.095476150512695, 4.101005554199219, 4.08567476272583, 4.082711219787598, 4.099205493927002, 4.088498115539551, 4.101178169250488, 4.101078987121582, 4.100683689117432, 4.098834037780762, 4.088640213012695, 4.102272033691406, 4.094213485717773, 4.10076379776001, 4.0982255935668945, 4.085578918457031, 4.0976243019104, 4.097197532653809, 4.100547790527344, 4.0997161865234375, 4.100890159606934, 4.089402675628662, 4.099499225616455, 4.102248668670654, 4.0959553718566895, 4.097997188568115, 4.1023149490356445, 4.095161437988281, 4.1007795333862305, 4.086809158325195, 4.097900390625, 4.1011834144592285, 4.102294921875, 4.080631732940674, 4.1023406982421875, 4.102357864379883, 4.101484298706055, 4.084290027618408, 4.098889350891113, 4.102189064025879, 4.099717140197754, 4.0989990234375, 4.08479118347168, 4.093035697937012, 4.0996599197387695, 4.102334976196289, 4.102152347564697, 4.100653171539307, 4.101735591888428, 4.094067096710205, 4.099870681762695, 4.101646423339844, 4.083113670349121, 4.095912933349609, 4.097222805023193, 4.086031913757324, 4.102222442626953, 4.101210594177246, 4.096536636352539, 4.10127592086792, 4.100471496582031, 4.097207069396973, 4.099720001220703, 4.0799078941345215, 4.101955890655518, 4.1021342277526855, 4.097733497619629, 4.085925102233887, 4.101797103881836, 4.098345756530762, 4.097221374511719, 4.098695755004883, 4.101142883300781, 4.102328300476074, 4.092353820800781, 4.094318389892578, 4.088315010070801, 4.081643104553223, 4.082149028778076, 4.1018524169921875, 4.099842071533203, 4.0997748374938965, 4.099106788635254, 4.083075523376465, 4.0953545570373535, 4.10200309753418, 4.101527214050293, 4.095795154571533, 4.102125644683838, 4.08662223815918, 4.101749897003174, 4.100580215454102, 4.102199077606201, 4.086718559265137, 4.101789474487305, 4.089043140411377, 4.102190971374512, 4.101160526275635, 4.102304458618164, 4.101950645446777, 4.102214813232422, 4.101619720458984, 4.101317405700684, 4.080631732940674, 4.084056854248047, 4.086946964263916, 4.0972394943237305, 4.098921298980713, 4.102135181427002, 4.09954309463501, 4.100559711456299, 4.080222129821777, 4.087123394012451, 4.1014604568481445, 4.101500988006592, 4.087827682495117, 4.100921630859375, 4.084850311279297, 4.098767280578613, 4.101395606994629, 4.0868096351623535, 4.1008710861206055, 4.096058368682861, 4.099761962890625, 4.099267959594727, 4.101281642913818, 4.101115703582764, 4.0962629318237305, 4.100162982940674, 4.100151062011719, 4.100616931915283, 4.093060493469238, 4.101211071014404, 4.101737976074219, 4.0898332595825195, 4.098376274108887, 4.086289405822754, 4.097812652587891, 4.1016130447387695, 4.095874786376953, 4.09796142578125, 4.082172870635986, 4.099277973175049, 4.0980753898620605, 4.098960876464844, 4.099544525146484, 4.100923538208008, 4.099453926086426, 4.089583396911621, 4.101986885070801, 4.090847015380859, 4.0970354080200195, 4.097310543060303, 4.102288246154785, 4.100736141204834, 4.098113059997559, 4.099536418914795, 4.091704368591309, 4.095951080322266, 4.090882301330566, 4.09843111038208, 4.090105056762695, 4.09381103515625, 4.084537029266357, 4.099998474121094, 4.099196434020996, 4.097675323486328, 4.1000871658325195, 4.097235679626465, 4.0898237228393555, 4.099727630615234, 4.098641395568848, 4.101749420166016, 4.0977463722229, 4.090031623840332, 4.102349281311035, 4.09895658493042, 4.092694282531738, 4.099791049957275, 4.102349281311035, 4.098387241363525, 4.100974082946777, 4.084268569946289, 4.097489833831787, 4.099159240722656, 4.101203441619873, 4.100553512573242, 4.099120140075684, 4.102251052856445, 4.101956367492676, 4.101441383361816, 4.102163314819336, 4.102313041687012, 4.102049350738525, 4.1018476486206055, 4.101593017578125, 4.096138000488281, 4.09754753112793, 4.099021911621094, 4.101968765258789, 4.1002655029296875, 4.084505081176758, 4.0943827629089355, 4.097014904022217, 4.10214376449585, 4.097613334655762, 4.088890075683594, 4.095378875732422, 4.098304748535156, 4.101998805999756, 4.101024627685547, 4.102193832397461, 4.0995283126831055, 4.098433494567871, 4.101193904876709, 4.101651668548584, 4.088435173034668, 4.088494300842285, 4.091897010803223, 4.099821090698242, 4.08579683303833, 4.084185600280762, 4.083608627319336, 4.093253135681152, 4.1014933586120605, 4.101606369018555, 4.1016740798950195, 4.097879886627197, 4.098759651184082, 4.102355480194092, 4.090085029602051, 4.088689804077148, 4.101980209350586, 4.102105140686035, 4.102055549621582, 4.102038860321045, 4.098738670349121, 4.1023149490356445, 4.102137565612793, 4.088231086730957, 4.0940093994140625, 4.102281093597412, 4.098574638366699, 4.0816521644592285, 4.1009297370910645, 4.096209526062012, 4.097471237182617, 4.099548816680908, 4.101408958435059, 4.0882720947265625, 4.099026679992676, 4.102027893066406, 4.084649085998535, 4.093206882476807, 4.099231719970703, 4.102300643920898, 4.085223197937012, 4.099617958068848, 4.099662780761719, 4.0833210945129395, 4.098939895629883, 4.084458351135254, 4.091229438781738, 4.10228967666626, 4.0817060470581055, 4.099924564361572, 4.093873023986816, 4.0982184410095215, 4.102059364318848, 4.1023173332214355, 4.084278106689453, 4.101006031036377, 4.101215362548828, 4.080309867858887, 4.098363876342773, 4.082873344421387, 4.100893974304199, 4.09142541885376, 4.10167121887207, 4.1020636558532715, 4.102187156677246, 4.102341651916504, 4.084244728088379, 4.094142436981201, 4.101729393005371, 4.09769344329834, 4.082962512969971, 4.101407051086426, 4.1011247634887695, 4.102278709411621, 4.100408554077148, 4.081256866455078, 4.090123653411865, 4.097705364227295, 4.082854747772217, 4.100307941436768, 4.097727298736572, 4.097620010375977, 4.101439476013184, 4.094086647033691, 4.083648681640625, 4.102357864379883, 4.093541145324707, 4.100168228149414, 4.102266311645508, 4.102334022521973, 4.089524745941162, 4.100747108459473, 4.097625255584717, 4.101401329040527, 4.087728500366211, 4.098777770996094, 4.102033615112305, 4.080654621124268, 4.098079204559326, 4.088134765625, 4.102254867553711, 4.100113868713379, 4.100374221801758, 4.09805154800415, 4.082255840301514, 4.081301689147949, 4.102344989776611, 4.099462509155273, 4.097675800323486, 4.095024585723877, 4.102324485778809, 4.098794937133789, 4.101160049438477, 4.093755722045898, 4.102325916290283, 4.098592758178711, 4.0881428718566895, 4.102209091186523, 4.100227355957031, 4.101125717163086, 4.101406574249268, 4.102298736572266, 4.1022138595581055, 4.102221488952637, 4.101848602294922, 4.100617408752441, 4.098989963531494, 4.092414379119873, 4.096487998962402, 4.092205047607422, 4.082647800445557, 4.09092903137207, 4.10109806060791, 4.0977349281311035, 4.094396591186523, 4.079965591430664, 4.086798667907715, 4.087384223937988, 4.101982593536377, 4.100709438323975, 4.1022796630859375, 4.091571807861328, 4.099306106567383, 4.082557678222656, 4.0943193435668945, 4.101163864135742, 4.099864959716797, 4.099557876586914, 4.087828636169434, 4.102054595947266, 4.1008453369140625, 4.102357864379883, 4.1001667976379395, 4.100893020629883, 4.080869674682617, 4.102280616760254, 4.101027965545654, 4.098519325256348, 4.090755462646484, 4.09992790222168, 4.100496292114258, 4.090627670288086, 4.102150917053223, 4.098574638366699, 4.091379642486572, 4.101238250732422, 4.097606658935547, 4.102114677429199, 4.080650806427002, 4.101940631866455, 4.102303504943848, 4.099706649780273, 4.100850582122803, 4.090950012207031, 4.09575891494751, 4.10028076171875, 4.10175895690918, 4.1007537841796875, 4.091086387634277, 4.089295387268066, 4.101910591125488, 4.102228164672852, 4.079922199249268, 4.084697723388672, 4.086800575256348, 4.097715377807617, 4.101559162139893, 4.101320743560791, 4.097894668579102, 4.099375247955322, 4.0955047607421875, 4.083839416503906, 4.083114147186279, 4.102003574371338, 4.084456443786621, 4.088383674621582, 4.091098308563232, 4.081806659698486, 4.100998401641846, 4.091868877410889, 4.08958625793457, 4.099874019622803, 4.097366809844971, 4.101201057434082, 4.1005539894104, 4.102173805236816, 4.102356910705566, 4.102326393127441, 4.097782135009766, 4.080247402191162, 4.10227108001709, 4.102357864379883, 4.101064205169678, 4.102304458618164, 4.101564884185791, 4.100004196166992, 4.095701694488525, 4.100817680358887, 4.102350234985352, 4.087401390075684, 4.090474605560303, 4.100980281829834, 4.099599838256836, 4.095277786254883, 4.101430416107178, 4.091638565063477, 4.084413051605225, 4.102262496948242, 4.087283134460449, 4.0982866287231445, 4.08260440826416]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}